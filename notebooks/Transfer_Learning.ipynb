{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d5c6b0-5032-4dff-a423-a1a792da9e63",
   "metadata": {},
   "source": [
    "### [Transfer Learning in Time Series Analysis](https://medium.com/@kylejones_47003/transfer-learning-in-time-series-analysis-4b7f1d1f4bfd)\n",
    "\n",
    "> Modern neural networks can learn temporal patterns from one domain and apply them to another, dramatically reducing the data needed for accurate predictions. This transfer of knowledge enables organizations to leverage existing models for new applications, from energy forecasting to healthcare monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcc3a2-522c-4219-a4f6-2f3bdbe5791b",
   "metadata": {},
   "source": [
    "Transfer learning represents a paradigm shift in how we approach time series modeling. Traditional time series analysis requires substantial data from the specific domain of interest.\n",
    "\n",
    "The application of transfer learning to time series data operates through several key mechanisms. Feature-based transfer learning extracts meaningful representations from source time series data that can be applied to target domains.\n",
    "\n",
    "Parameter-based transfer learning, alternatively, reuses parts of a trained modelâ€™s architecture or parameters, fine-tuning them for the new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72f4b5d-e6ba-454c-9222-c07cfdb9c354",
   "metadata": {},
   "source": [
    "##### Instance-based Transfer Learning\n",
    "\n",
    "Instance-based transfer learning selectively uses samples from the source domain to augment learning in the target domain. This approach proves particularly valuable when dealing with rare events or anomalies in time series data.\n",
    "\n",
    "The key challenge lies in identifying which instances from the source domain remain relevant to the target problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f3c9e-b8b0-41d6-a5a5-9bf99d51d794",
   "metadata": {},
   "source": [
    "##### Deep Transfer Learning for Time Series\n",
    "\n",
    "Deep learning architectures have dramatically expanded the possibilities for transfer learning in time series analysis. Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks can learn hierarchical representations of temporal patterns that often generalize across domains.\n",
    "\n",
    "A model initially trained on high-frequency financial data might extract features useful for analyzing medical time series, despite the apparent differences between these domains. The deep learning approach to transfer learning often involves freezing early layers of the network while retraining later layers on the target domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543dbed-6281-4a9c-b22c-73168694333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas matplotlib\n",
    "!pip install -q scikit-learn tensorflow==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e7d11-7fd1-41f1-9b63-7afa74c846b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c6db2-0895-4e97-a70a-81ccd08dbc90",
   "metadata": {},
   "source": [
    "#### Basic Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c4ec7-f6cd-4df3-b5bf-0226c0b75f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "# Helper function to create time series sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:(i + seq_length)])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Load and prepare source domain data (e.g., energy consumption)\n",
    "source_data = pd.read_csv('energy_consumption.csv')\n",
    "source_scaler = MinMaxScaler()\n",
    "source_scaled = source_scaler.fit_transform(source_data[['consumption']])\n",
    "source_sequences = create_sequences(source_scaled, seq_length=24)\n",
    "\n",
    "# Load and prepare target domain data (e.g., solar production)\n",
    "target_data = pd.read_csv('solar_production.csv')\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaled = target_scaler.fit_transform(target_data[['production']])\n",
    "target_sequences = create_sequences(target_scaled, seq_length=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c103499-c443-49b4-a2a7-6997b2a69392",
   "metadata": {},
   "source": [
    "#### Building a Base Model for Source Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ece568-39b8-447c-b196-d4ccf20602ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_model(sequence_length, n_features=1):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(sequence_length, n_features), return_sequences=True),\n",
    "        LSTM(32),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train base model on source domain\n",
    "source_model = create_base_model(24)\n",
    "source_model.fit(\n",
    "    source_sequences[:-1], \n",
    "    source_scaled[24:], \n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cec007-00d8-4f03-9457-e50a4534f165",
   "metadata": {},
   "source": [
    "#### Feature-based Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061eb92e-4ec3-4c21-a9de-9d258e76efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from intermediate layer\n",
    "def create_feature_extractor(base_model, layer_name='lstm_1'):\n",
    "    return Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=base_model.get_layer(layer_name).output\n",
    "    )\n",
    "\n",
    "feature_extractor = create_feature_extractor(source_model)\n",
    "\n",
    "# Create new model using transferred features\n",
    "def create_transfer_model(feature_extractor, sequence_length):\n",
    "    inputs = Input(shape=(sequence_length, 1))\n",
    "    features = feature_extractor(inputs)\n",
    "    x = LSTM(16)(features)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "transfer_model = create_transfer_model(feature_extractor, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244015cd-ba73-4f4a-b929-3fffa62d525b",
   "metadata": {},
   "source": [
    "#### Fine-tuning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a7d2d1-ea09-4bac-98a2-ec2066cbe48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fine_tuning_model(base_model, trainable_layers=1):\n",
    "    # Freeze early layers\n",
    "    for layer in base_model.layers[:-trainable_layers]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return base_model\n",
    "\n",
    "# Clone source model for fine-tuning\n",
    "fine_tune_model = tf.keras.models.clone_model(source_model)\n",
    "fine_tune_model.set_weights(source_model.get_weights())\n",
    "fine_tune_model = create_fine_tuning_model(fine_tune_model)\n",
    "\n",
    "# Fine-tune on target domain\n",
    "fine_tune_model.fit(\n",
    "    target_sequences[:-1],\n",
    "    target_scaled[24:],\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c1a501-9c03-4df3-834d-bb7683138c7c",
   "metadata": {},
   "source": [
    "#### Domain Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f5223-5908-4ff7-8c5e-a46b41d709a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DomainAdapter:\n",
    "    def __init__(self, source_scaler, target_scaler):\n",
    "        self.source_scaler = source_scaler\n",
    "        self.target_scaler = target_scaler\n",
    "    \n",
    "    def adapt_sequence(self, sequence, from_domain='source', to_domain='target'):\n",
    "        if from_domain == 'source' and to_domain == 'target':\n",
    "            # Inverse transform to original scale\n",
    "            sequence_orig = self.source_scaler.inverse_transform(sequence)\n",
    "            # Transform to target scale\n",
    "            return self.target_scaler.transform(sequence_orig)\n",
    "        else:\n",
    "            sequence_orig = self.target_scaler.inverse_transform(sequence)\n",
    "            return self.source_scaler.transform(sequence_orig)\n",
    "\n",
    "# Create and use domain adapter\n",
    "adapter = DomainAdapter(source_scaler, target_scaler)\n",
    "adapted_sequences = adapter.adapt_sequence(source_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4537830-afee-412f-b785-3765607d9dfe",
   "metadata": {},
   "source": [
    "#### Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed7d78-223e-49c1-bb0c-3ffca5dcdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, test_sequences, test_targets):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        predictions = model.predict(test_sequences)\n",
    "        mse = tf.keras.losses.MSE(test_targets, predictions)\n",
    "        mae = tf.keras.losses.MAE(test_targets, predictions)\n",
    "        results[name] = {'MSE': float(mse), 'MAE': float(mae)}\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Compare different approaches\n",
    "models = {\n",
    "    'Base Model': source_model,\n",
    "    'Transfer Learning': transfer_model,\n",
    "    'Fine-tuned': fine_tune_model\n",
    "}\n",
    "\n",
    "results = evaluate_models(\n",
    "    models,\n",
    "    target_sequences[-100:],\n",
    "    target_scaled[-100:]\n",
    ")\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f6842-d74a-467c-ba5d-c1c34aa27d09",
   "metadata": {},
   "source": [
    "#### Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3671ef-8618-4191-9776-b109a217990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_predictions(models, test_sequences, true_values, scaler):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot true values\n",
    "    plt.plot(scaler.inverse_transform(true_values), \n",
    "             label='Actual', linewidth=2)\n",
    "    \n",
    "    # Plot predictions from each model\n",
    "    for name, model in models.items():\n",
    "        predictions = model.predict(test_sequences)\n",
    "        plt.plot(scaler.inverse_transform(predictions), \n",
    "                label=f'{name} Predictions', linestyle='--')\n",
    "    \n",
    "    plt.title('Model Predictions Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize results\n",
    "plot_predictions(\n",
    "    models,\n",
    "    target_sequences[-100:],\n",
    "    target_scaled[-100:],\n",
    "    target_scaler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51fdb0c-3aea-46d7-b554-92ca779f20c8",
   "metadata": {},
   "source": [
    "#### Best Practices and Implementation Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bade836-8361-4f0b-b427-8b2580d6a2ca",
   "metadata": {},
   "source": [
    "- First, source and target domains should share meaningful similarities in their temporal patterns or underlying generative processes.\n",
    "- Second, the transfer learning approach should account for differences in scale, sampling frequency, and noise levels between domains.\n",
    "- Third, validation strategies must carefully assess whether the transferred knowledge improves or potentially degrades performance in the target domain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
