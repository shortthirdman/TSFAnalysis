{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e87612-e1ba-4553-bedc-ff69a4b635b5",
   "metadata": {},
   "source": [
    "## [Sundail (LLM) versus Temporal Convolutional Networks (TCN) for Time Series Forecasting of Crude Oil Pricing](https://medium.com/@kylejones_47003/sundail-llm-versus-temporal-convolutional-networks-tcn-for-time-series-forecasting-of-crude-oil-a954d5e2f9d2)\n",
    "\n",
    "Sundial is a causal transformer designed for time series. It treats the sequence like a language modeling task. The TCN is a 1D convolutional model with a single kernel layer, ReLU, and linear output.\n",
    "\n",
    "Sundial handles uncertainty well and provides a probabilistic forecast. The TCN responds more directly to recent fluctuations and produces a tighter prediction. Both learn meaningful structure from the same inputs. Both capture broad seasonal shape. Their residuals differ in character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8158bcee-96fa-4439-9678-359dec9dbced",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas scikit-learn matplotlib\n",
    "!pip install -q pandas-datareader torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c84e74-3083-41c1-9a95-5e4b5f5622fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f2a1e07-fd02-4c60-92c0-5dc8edd7734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from pandas_datareader.data import DataReader\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7e5dec-ccb2-4671-a1b8-b23bc0d98534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fred_data(series_id, start='2000-01-01'):\n",
    "    df = DataReader(series_id, 'fred', start)\n",
    "    df = df.rename(columns={series_id: 'value'})\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "def compute_mape(y_true, y_pred):\n",
    "    eps = 1e-8\n",
    "    ape = torch.abs((y_true - y_pred) / (y_true + eps))\n",
    "    ape = torch.where(torch.isinf(ape), torch.zeros_like(ape), ape)\n",
    "    return ape.mean().item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d195775-e4c6-45a1-b8ee-97f0a4320c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.mean(dim=2)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac25411f-9560-489a-b668-5f95a12ff6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 63.84029563585172\n",
      "Standard Deviation: 25.230146245915975\n"
     ]
    }
   ],
   "source": [
    "# Reload everything and redo forecasting\n",
    "df = fetch_fred_data('DCOILWTICO')\n",
    "df = df.set_index('DATE').resample('D').mean().dropna()\n",
    "\n",
    "raw_series = df['value']\n",
    "mean = raw_series.mean()\n",
    "std = raw_series.std()\n",
    "series = (raw_series - mean) / std\n",
    "series = series.dropna()\n",
    "\n",
    "lookback_length = 256\n",
    "forecast_length = 96\n",
    "num_samples = 20\n",
    "\n",
    "train_values = series.values[-(lookback_length + forecast_length + 100):-(forecast_length)]\n",
    "true_future = raw_series.values[-forecast_length:]\n",
    "true = torch.tensor(true_future)\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Standard Deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0139a23e-46b0-407e-8d0a-9fc56f219c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicCache' object has no attribute 'get_max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m input_tensor = torch.tensor(train_values[-lookback_length:], dtype=torch.float32).unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m      3\u001b[39m sundial = AutoModelForCausalLM.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mthuml/sundial-base-128m\u001b[39m\u001b[33m'\u001b[39m, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m sundial_samples = [\u001b[43msundial\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforecast_length\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples)]\n\u001b[32m      5\u001b[39m sundial_output = torch.stack(sundial_samples)\n\u001b[32m      7\u001b[39m sundial_pred_norm = sundial_output.mean(dim=\u001b[32m0\u001b[39m).squeeze()[:forecast_length]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\TimeSeriesForecasting\\dev\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\thuml\\sundial-base-128m\\1cb07e76fc4e76a074a278f56fa4d8dd30639895\\ts_generation_mixin.py:34\u001b[39m, in \u001b[36mTSGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, revin, num_samples, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m     stdev = inputs.std(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m, unbiased=\u001b[38;5;28;01mFalse\u001b[39;00m) + \u001b[32m1e-5\u001b[39m\n\u001b[32m     33\u001b[39m     inputs = (inputs - means) / stdev\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix_allowed_tokens_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massistant_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnegative_prompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m revin:\n\u001b[32m     36\u001b[39m     stdev = stdev.unsqueeze(\u001b[32m1\u001b[39m).repeat(\u001b[32m1\u001b[39m, num_samples, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\TimeSeriesForecasting\\dev\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\TimeSeriesForecasting\\dev\\Lib\\site-packages\\transformers\\generation\\utils.py:2623\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2615\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2616\u001b[39m         input_ids=input_ids,\n\u001b[32m   2617\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2618\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2619\u001b[39m         **model_kwargs,\n\u001b[32m   2620\u001b[39m     )\n\u001b[32m   2622\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2623\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2624\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2634\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2635\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2636\u001b[39m         input_ids=input_ids,\n\u001b[32m   2637\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2638\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2639\u001b[39m         **model_kwargs,\n\u001b[32m   2640\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\WORKSPACE\\GitHub\\shortthirdman\\Jupyter Notebooks\\TimeSeriesForecasting\\dev\\Lib\\site-packages\\transformers\\generation\\utils.py:3597\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3593\u001b[39m     is_prefill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3595\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n\u001b[32m   3596\u001b[39m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3597\u001b[39m     model_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_inputs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3599\u001b[39m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n\u001b[32m   3600\u001b[39m     model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_attentions\u001b[39m\u001b[33m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.cache\\huggingface\\modules\\transformers_modules\\thuml\\sundial-base-128m\\1cb07e76fc4e76a074a278f56fa4d8dd30639895\\modeling_sundial.py:522\u001b[39m, in \u001b[36mSundialForPrediction.prepare_inputs_for_generation\u001b[39m\u001b[34m(self, input_ids, past_key_values, attention_mask, inputs_embeds, revin, num_samples, **kwargs)\u001b[39m\n\u001b[32m    519\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    520\u001b[39m         past_length = cache_length\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m     max_cache_length = \u001b[43mpast_key_values\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_max_length\u001b[49m()\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    524\u001b[39m     cache_length = past_length = past_key_values[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].shape[\u001b[32m2\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicCache' object has no attribute 'get_max_length'"
     ]
    }
   ],
   "source": [
    "# Sundial\n",
    "input_tensor = torch.tensor(train_values[-lookback_length:], dtype=torch.float32).unsqueeze(0)\n",
    "sundial = AutoModelForCausalLM.from_pretrained('thuml/sundial-base-128m', trust_remote_code=True)\n",
    "sundial_samples = [sundial.generate(input_tensor, max_new_tokens=forecast_length) for _ in range(num_samples)]\n",
    "sundial_output = torch.stack(sundial_samples)\n",
    "\n",
    "sundial_pred_norm = sundial_output.mean(dim=0).squeeze()[:forecast_length]\n",
    "sundial_pred = sundial_pred_norm * std + mean\n",
    "lower = sundial_output.quantile(0.10, dim=0).squeeze()[:forecast_length] * std + mean\n",
    "upper = sundial_output.quantile(0.90, dim=0).squeeze()[:forecast_length] * std + mean\n",
    "\n",
    "print(f\"Sundail Predictions: {sundial_pred}\")\n",
    "print(f\"Lower Quantile: {lower}\")\n",
    "print(f\"Upper Quantile: {upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c037594-8672-4543-aa8f-1444ce0878e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCN\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(train_values) - lookback_length - forecast_length):\n",
    "    X_seq.append(train_values[i:i+lookback_length])\n",
    "    y_seq.append(train_values[i+lookback_length:i+lookback_length+forecast_length])\n",
    "\n",
    "X_seq = torch.tensor(X_seq, dtype=torch.float32).unsqueeze(1)\n",
    "y_seq = torch.tensor(y_seq, dtype=torch.float32)\n",
    "loader = DataLoader(TensorDataset(X_seq, y_seq), batch_size=16, shuffle=True)\n",
    "\n",
    "tcn_model = TCN(input_size=1, hidden_size=32, output_size=forecast_length)\n",
    "optimizer = torch.optim.Adam(tcn_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    tcn_model.train()\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = tcn_model(xb)\n",
    "        loss = loss_fn(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "tcn_model.eval()\n",
    "last_input = torch.tensor(train_values[-lookback_length:], dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "tcn_pred_norm = tcn_model(last_input).detach().squeeze()\n",
    "tcn_pred = tcn_pred_norm * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c99104f-440c-4f55-95f0-f71d5241b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "def plot_full_forecast(df, true, sundial_pred, tcn_pred, lower, upper, forecast_length):\n",
    "    past_2y = df[-(730 + forecast_length):-forecast_length]\n",
    "    future_dates = df.index[-forecast_length:]\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(past_2y.index, past_2y.values, label=\"Historical (2y)\", linewidth=1.8)\n",
    "    plt.plot(future_dates, true.numpy(), label=\"True\", linewidth=2)\n",
    "    plt.plot(future_dates, sundial_pred.detach().numpy(), label=\"Sundial\", linestyle=\"--\")\n",
    "    plt.plot(future_dates, tcn_pred.detach().numpy(), label=\"TCN\", linestyle=\":\")\n",
    "    plt.fill_between(future_dates,\n",
    "                     lower.detach().numpy(),\n",
    "                     upper.detach().numpy(),\n",
    "                     color='blue', alpha=0.2, label=\"Sundial 80% CI\")\n",
    "    plt.title(\"Two-Year Historical Data with Forecast Comparison\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    ## plt.savefig(\"forecast_2yr_plus_predictions.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697c135f-db2d-4312-b615-7986388c2ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sundial_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plot_full_forecast(df[\u001b[33m'\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m'\u001b[39m], true, \u001b[43msundial_pred\u001b[49m, tcn_pred, lower, upper, forecast_length)\n",
      "\u001b[31mNameError\u001b[39m: name 'sundial_pred' is not defined"
     ]
    }
   ],
   "source": [
    "plot_full_forecast(df['value'], true, sundial_pred, tcn_pred, lower, upper, forecast_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
