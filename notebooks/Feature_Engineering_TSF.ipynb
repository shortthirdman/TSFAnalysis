{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5a6d78-2248-4342-836a-1d8d0b447c46",
   "metadata": {},
   "source": [
    "### [Feature Engineering for Time Series Forecasting](https://python.plainenglish.io/feature-engineering-for-time-series-forecasting-in-python-7c469f69e260)\n",
    "\n",
    "Feature engineering is the process of creating additional input features from raw time series data to improve the performance of predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba55882-12d8-437c-b090-a5c1d0c2e08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpy pandas matplotlib\n",
    "!pip install -q scikit-learn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26f2a2f-a443-4451-b26e-ee9b00d6e87e",
   "metadata": {},
   "source": [
    "##### Scaling Values\n",
    "\n",
    "Scaling methods include:\n",
    "\n",
    "- Min-Max Scaling: Rescales values to a specific range, e.g., `[0, 1]`.\n",
    "\n",
    "- Standardization (Z-score): Rescales values to have a mean of 0 and standard deviation of 1.\n",
    "\n",
    "Use *Min-Max Scaling* for data where the range matters.\n",
    "\n",
    "Use *Standardization* when the scale is unknown or when working with models sensitive to variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd2d41d-c6f1-4b49-ab85-fd63ee93af7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled: [0.   0.25 0.5  0.75 1.  ]\n",
      "Standardized: [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "# Example Time Series\n",
    "y = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "y_minmax = scaler.fit_transform(y)\n",
    "# Standardization\n",
    "scaler_std = StandardScaler()\n",
    "y_std = scaler_std.fit_transform(y)\n",
    "print(\"Min-Max Scaled:\", y_minmax.flatten())\n",
    "print(\"Standardized:\", y_std.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66027012-8b51-4193-8c71-1f24dfc1e1be",
   "metadata": {},
   "source": [
    "##### Looking at Changes in Values\n",
    "\n",
    "Instead of analyzing absolute values, focusing on changes can remove trends and reveal stationarity.\n",
    "\n",
    "First-order differencing removes trends to make the data stationary and highlighting short-term changes in the series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a8d6ba0-b805-4eb3-9676-bc403aed188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Difference: [2. 3. 4. 5.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Example Time Series\n",
    "y = pd.Series([10, 12, 15, 19, 24])\n",
    "# First Difference\n",
    "y_diff = y.diff().dropna()\n",
    "print(\"First Difference:\", y_diff.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54838bf5-6cd4-441f-b379-3a0aba0984bc",
   "metadata": {},
   "source": [
    "##### Derivatives: Rate of Change and Acceleration\n",
    "Derivatives measure the rate of change in a time series, which can highlight momentum or acceleration patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d2805f-31b6-453f-a204-46af591c6352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Derivative (Rate of Change): [2.  2.5 3.5 4.5 5. ]\n",
      "Second Derivative (Acceleration): [0.5  0.75 1.   0.75 0.5 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Example Time Series\n",
    "y = np.array([10, 12, 15, 19, 24])\n",
    "# First Derivative (Rate of Change)\n",
    "dy = np.gradient(y)\n",
    "print(\"First Derivative (Rate of Change):\", dy)\n",
    "# Second Derivative (Acceleration)\n",
    "d2y = np.gradient(dy)\n",
    "print(\"Second Derivative (Acceleration):\", d2y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9905d-caf3-4ea1-8ca1-9ba6a2f926f1",
   "metadata": {},
   "source": [
    "*First derivatives* (measures the rate of change) help capture trends and momentum.\n",
    "\n",
    "*Second derivatives* (measures the rate of change of the rate of change) detect points of inflection or changes in acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97c6ff-fcd5-4be0-99f7-55dbbfb2186c",
   "metadata": {},
   "source": [
    "##### Embedding Prior Values: Building “Memory”\n",
    "Embedding previous observations as features allows models to “remember” past values. This is especially important for models that do not inherently capture temporal dependencies (e.g., regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0c5d100-04db-4f7e-955b-7a99f7f4d24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>lag_1</th>\n",
       "      <th>lag_2</th>\n",
       "      <th>rate_of_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.006138</td>\n",
       "      <td>0.358450</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>0.647689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.529168</td>\n",
       "      <td>1.006138</td>\n",
       "      <td>0.358450</td>\n",
       "      <td>1.523030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.295015</td>\n",
       "      <td>2.529168</td>\n",
       "      <td>1.006138</td>\n",
       "      <td>-0.234153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.060878</td>\n",
       "      <td>2.295015</td>\n",
       "      <td>2.529168</td>\n",
       "      <td>-0.234137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.640091</td>\n",
       "      <td>2.060878</td>\n",
       "      <td>2.295015</td>\n",
       "      <td>1.579213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value     lag_1     lag_2  rate_of_change\n",
       "2  1.006138  0.358450  0.496714        0.647689\n",
       "3  2.529168  1.006138  0.358450        1.523030\n",
       "4  2.295015  2.529168  1.006138       -0.234153\n",
       "5  2.060878  2.295015  2.529168       -0.234137\n",
       "6  3.640091  2.060878  2.295015        1.579213"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Simulated Time Series Data\n",
    "np.random.seed(42)\n",
    "data = pd.Series(np.cumsum(np.random.randn(200))) # Random walk time series\n",
    "# Create Features: Lagged Values and Rate of Change\n",
    "df = pd.DataFrame({\n",
    "'value': data,\n",
    "'lag_1': data.shift(1),\n",
    "'lag_2': data.shift(2),\n",
    "'rate_of_change': data.diff()\n",
    "}).dropna()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2359ba33-1a11-476b-8294-ec1ac985f7bf",
   "metadata": {},
   "source": [
    "##### Rolling Statistics\n",
    "Calculate rolling means, variances, or medians over a window to smooth the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fa54145-4468-4b0f-b4f4-8c39f26bef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Mean: 0          NaN\n",
      "1          NaN\n",
      "2    12.333333\n",
      "3    15.333333\n",
      "4    19.333333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Example Time Series\n",
    "y = pd.Series([10, 12, 15, 19, 24])\n",
    "rolling_mean = y.rolling(window=3).mean()\n",
    "print(\"Rolling Mean:\", rolling_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294517f-0d37-43dc-88a8-b8cf636741f3",
   "metadata": {},
   "source": [
    "##### Extracting Seasonality\n",
    "\n",
    "Decompose a series into trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992168c-936e-4d3e-b3bd-fbcc8de96893",
   "metadata": {},
   "source": [
    "```python\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "result = seasonal_decompose(y, period=12)\n",
    "result.seasonal.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede77d63-c992-48d3-b703-cf053e4b6b60",
   "metadata": {},
   "source": [
    "##### Fourier Transforms\n",
    "Use Fourier transformations to identify dominant frequencies in seasonal data.\n",
    "\n",
    "##### Time-Based Features\n",
    "Extract calendar-related features like month, day of the week, or hour to capture seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801fa9b-0e80-43ab-8b73-a4328e360b60",
   "metadata": {},
   "source": [
    "###### Adding Time Features\n",
    "```python\n",
    "df.index = pd.date_range(start=\"2023–01\", periods=len(df), freq=\"M\")\n",
    "df_features = pd.DataFrame({\"month\": df.index.month, \"year\": df.index.year})\n",
    "print(df_features.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26735bb7-ef15-4cab-990a-19566874b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ERCOT data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/jgscott/ECO395M/refs/heads/master/data/ercot/load_data.csv\")\n",
    "df['date'] = pd.to_datetime(df['Time'])  # Ensure 'date' is in datetime format\n",
    "df['values'] = pd.to_numeric(df['ERCOT'], errors='coerce')  # Convert 'values' to numeric\n",
    "df = df.sort_values('date')  # Sort by date\n",
    "\n",
    "# Drop rows with missing or NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Resample the data to hourly frequency (mean aggregation)\n",
    "df = df.set_index('date').resample('h').mean().reset_index()  # Resample to hourly frequency\n",
    "\n",
    "# Define hold-out period (e.g., last 24 hours)\n",
    "hold_out_hours = 24  # Hold-out size (24 hours = 1 day)\n",
    "train = df.iloc[:-hold_out_hours]\n",
    "hold_out = df.iloc[-hold_out_hours:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027eff29-846d-4aef-a768-dc479bbb8fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Ensure train and hold_out datasets are already defined\n",
    "# Assuming train and hold_out data are from the split\n",
    "\n",
    "# Scaling: Min-Max and Standardization (for train data only)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "scaler_std = StandardScaler()\n",
    "\n",
    "train_values = train['values'].values.reshape(-1, 1)  # Reshape for scalers\n",
    "values_scaled_minmax = scaler_minmax.fit_transform(train_values)\n",
    "values_scaled_std = scaler_std.fit_transform(train_values)\n",
    "\n",
    "print(\"Min-Max Scaled Train Values:\\n\", values_scaled_minmax.flatten())\n",
    "print(\"Standardized Train Values:\\n\", values_scaled_std.flatten())\n",
    "\n",
    "# First Difference (for train data only)\n",
    "values_diff = train['values'].diff().dropna()\n",
    "print(\"First Difference:\\n\", values_diff.head())\n",
    "\n",
    "# First and Second Derivative (Rate of Change and Acceleration) (for train data only)\n",
    "values_gradient = np.gradient(train['values'].values)\n",
    "values_acceleration = np.gradient(values_gradient)\n",
    "print(\"First Derivative (Rate of Change):\\n\", values_gradient[:5])\n",
    "print(\"Second Derivative (Acceleration):\\n\", values_acceleration[:5])\n",
    "\n",
    "# Rolling Mean (for train data only)\n",
    "rolling_mean = train['values'].rolling(window=3).mean()\n",
    "print(\"Rolling Mean (window=3):\\n\", rolling_mean.head())\n",
    "\n",
    "# Seasonal Decomposition (for train data only)\n",
    "seasonal_decomposition = seasonal_decompose(train['values'], period=12, model='additive')\n",
    "print(\"Seasonal Component Head:\\n\", seasonal_decomposition.seasonal.head())\n",
    "\n",
    "# Create Lagged Features and Rate of Change (for train data only)\n",
    "df_features_train = pd.DataFrame({\n",
    "    'value': train['values'],\n",
    "    'lag_1': train['values'].shift(1),\n",
    "    'lag_2': train['values'].shift(2),\n",
    "    'rate_of_change': train['values'].diff()\n",
    "}).dropna()\n",
    "\n",
    "# Adding Time-Based Features (for train data only)\n",
    "df_features_train['month'] = train['date'].dt.month\n",
    "df_features_train['year'] = train['date'].dt.year\n",
    "print(\"Time-Based Features (Train Data):\\n\", df_features_train[['month', 'year']].head())\n",
    "\n",
    "# Save rolling mean plot for train data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train['date'], train['values'], label=\"Train Values\", color='Blue')\n",
    "plt.plot(train['date'], rolling_mean, label=\"Rolling Mean (window=3)\", color='Red')\n",
    "\n",
    "plt.plot(hold_out['date'], hold_out['values'], label=\"Hold-Out Values\", color='Green')\n",
    "plt.title(\"Hold-Out Values\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"holdout_values_ercot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c5b7a-4a53-40e9-9111-6c621376728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# Fit an Exponential Smoothing model on the training data\n",
    "# Adjust seasonal_periods and trend/seasonal type as per data characteristics\n",
    "model = ExponentialSmoothing(\n",
    "    train['values'],\n",
    "    trend=\"additive\",\n",
    "    seasonal=\"additive\",\n",
    "    seasonal_periods=24  # Assuming daily seasonality in hourly data\n",
    ")\n",
    "fitted_model = model.fit()\n",
    "\n",
    "# Forecast values for the hold-out period\n",
    "forecast_values = fitted_model.forecast(steps=len(hold_out))\n",
    "\n",
    "# Add forecast to the hold_out DataFrame for easier plotting\n",
    "hold_out['forecast'] = forecast_values.values\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape_value = mean_absolute_percentage_error(hold_out['values'], hold_out['forecast'])\n",
    "print(f\"MAPE: {mape_value * 100:.2f}%\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training data\n",
    "plt.plot(train['date'], train['values'], label=\"Training Data\", color='blue')\n",
    "\n",
    "# Plot hold-out data\n",
    "plt.plot(hold_out['date'], hold_out['values'], label=\"Hold-Out Data (Actual)\", color='green')\n",
    "\n",
    "# Plot forecasted data\n",
    "plt.plot(hold_out['date'], hold_out['forecast'], label=\"Forecast\", color='red', linestyle=\"--\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(f\"Forecasting Hold-Out Data \\n MAPE: {mape_value * 100:.2f}%\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Load Values\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "# plt.savefig(\"forecast_holdout_ercot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
